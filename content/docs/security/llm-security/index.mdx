---
title: 大模型安全
description: 提供进入大语言模型安全领域的路线图和资源推荐
---

> **提醒**：下文内容由AI根据大纲生成
> **最后部分有学长自己的一些 Insights**: 

## 引言

人工智能，特别是大语言模型（LLM）技术正以前所未有的速度发展，并深刻地改变着各行各业。伴随其广泛应用而来的是全新的安全挑战。对于网络安全专业人士而言，将现有技能扩展至AI安全领域，不仅是技术能力的提升，更是顺应时代发展的必然选择。

本文旨在为您提供一份清晰、循序渐进的学习路线图，帮助您系统地进入大模型安全这一前沿领域。

## 第一步：夯实理论基础——理解LLM的工作原理

在深入安全问题之前，您需要对LLM背后的基本原理有直观的理解。这并非要求您成为算法专家，而是为了让您在后续的安全研究中能知其然，更知其所以然。

**推荐资源**：[3Blue1Brown的神经网络系列课程](https://www.3blue1brown.com/topics/neural-networks)

**为什么推荐**？ 这是目前公认的最清晰、最直观的神经网络入门讲解。它通过精妙的可视化动画，将复杂的数学概念转化为易于理解的图像和逻辑，是建立心智模型的绝佳起点。

## 第二步：熟悉核心平台——掌握与LLM交互的方式

理论之后，您需要亲身实践。与LLM的交互主要分为两种：通过应用界面（如ChatGPT网站）和通过API进行程序调用。后者是构建、集成和测试LLM应用安全性的关键。

**推荐资源 1**：[Hugging Face](https://huggingface.co/)

**为什么推荐**？ Hugging Face是AI领域的GitHub，是开源模型、数据集和应用的聚集地。花时间熟悉其生态，您将学会如何查找、下载、运行和评估不同的模型，这是进行安全研究的基础。

**推荐资源 2**：[OpenRouter](https://openrouter.ai/)

**为什么推荐**？ 要想通过API与主流模型（如GPT系列、Claude系列）交互，通常需要付费。OpenRouter聚合了众多模型，并为新用户提供免费试用模型（国内前三的最新模型都有）。它是您在零成本下学习和测试API调用的理想平台。

## 第三步：掌握安全框架——建立系统性认知

了解了基本原理和交互方式后，便可以正式进入安全领域。学习业界公认的安全框架，是系统化掌握LLM安全威胁的捷径。

**核心必读**：[OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)

**为什么推荐**？ 这是LLM应用安全领域的"圣经"。它系统地梳理了最核心的十大安全风险，如提示注入（Prompt Injection）、不安全的输出处理（Insecure Output Handling）等。理解并掌握这份清单，是您构建LLM安全知识体系的基石。

**进阶学习**：[MITRE ATLAS™](https://atlas.mitre.org/)

**为什么推荐**？ 如果说OWASP Top 10定义了"有什么漏洞"，那么MITRE ATLAS则描绘了"攻击者如何利用这些漏洞"。该框架整理了针对AI系统的真实攻击战术、技术和流程（TTPs），帮助您从攻击者的视角理解AI面临的威胁。

## 第四步：动手实践攻防——将知识转化为技能

理论结合实践才能真正巩固所学。使用专门的工具对LLM进行安全测试（红队演练），是检验和提升技能的最佳方式。

**推荐工具**：[NVIDIA Garak](https://github.com/NVIDIA/garak)

**为什么推荐**？ Garak是一个强大的LLM漏洞扫描和安全评测工具。它内置了大量针对不同漏洞（如提示注入、越狱、敏感信息泄露等）的探测器。通过使用Garak，您可以亲手复现OWASP Top 10中描述的多种攻击，并直观感受不同模型的安全水位。

## 第五步：持续追踪前沿——融入社区保持更新

AI安全是一个飞速发展的领域，新的攻击手法和防御策略层出不穷。保持学习、紧跟社区是成为专家的必经之路。

**推荐方式**：在GitHub搜索 "[Awesome LLM Security](https://github.com/search?q=Awesome+LLM+Security)"

**为什么推荐**？ GitHub上有多个由社区维护的精选资源列表（Awesome Lists）。它们汇集了最新的研究论文、工具、文章和项目。通过关注这些列表，您可以高效地获取该领域的最新动态。

## 结语

从理解基本原理，到熟悉核心平台，再到掌握安全框架并动手实践，最后融入社区持续学习——这条路径将指引您稳步迈入AI安全的大门。这一领域充满了挑战与机遇，希望这份指南能成为您探索新知的有力起点。

如果您在学习过程中有任何疑问，或希望就特定主题进行更深入的探讨，欢迎随时交流。

---

以及这是我之前24年写的洞察：[AI Insights 2024](https://mundi-xu.github.io/2024/12/18/AI-Insights-2024/)，也可以参考一下。今年多了很多新东西，比如智能体、MCP等的发展搞出来很多间接注入的手法，年底有空再更新吧。大模型安全还是很好入门的，因为一些基础的比如越狱或者提示词注入等攻击足够简单且有趣，而且易得。但是这种“开箱即用”的攻击方式也会导致一种错误的认知，即认为这些攻击是无害的“技术游戏”。抛开违反平台服务条款可能导致封号不谈（应该也没几个人会严格遵守吧），恶意利用系统漏洞或是生成并传播非法内容等都是需要承担法律责任的。所以，在尝试那条“有趣”的prompt之前，确认自己是否已经了解并愿意承担它可能带来的所有后果，或者使用更安全合规的方式，比如在本地环境搭建开源模型进行测试。
